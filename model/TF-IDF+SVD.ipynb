{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Импорт библиотек\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "dHUL7BvTEp6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Отключаем предупреждения\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "CONFIG = {\n",
        "    'data': {\n",
        "        'train_path': \"/kaggle/input/train-dataset/train_v2_drcat_02.csv\",\n",
        "        'test_path': \"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\",\n",
        "        'random_state': 42,\n",
        "        'n_folds': 3\n",
        "    },\n",
        "    'tfidf': {\n",
        "        'max_features': 10000,\n",
        "        'ngram_range': (1, 2)\n",
        "    },\n",
        "    'svd': {\n",
        "        'n_components': 300\n",
        "    },\n",
        "    'model': {\n",
        "        'type': 'logistic',  # 'logistic', 'random_forest' или 'svm'\n",
        "        'params': {\n",
        "            'logistic': {'C': 1.0, 'max_iter': 1000},\n",
        "            'random_forest': {'n_estimators': 100, 'max_depth': 5},\n",
        "            'svm': {'C': 1.0, 'kernel': 'linear'}\n",
        "        }\n",
        "    }\n",
        "}\n"
      ],
      "metadata": {
        "id": "4ce9FaKpEvG4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка датасетов\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Расширенная очистка текста от артефактов\"\"\"\n",
        "    patterns = [\n",
        "        r'sincerely,\\s*\\[your name\\]',\n",
        "        r'as an (8th|eighth\\-grade) student',\n",
        "        r'(writing|today) to express',\n",
        "        r'hey there! so,',\n",
        "        r'first impressions are',\n",
        "        r'a four\\-day school week',\n",
        "        r'reduce traffic congestion[,\\\\.]',\n",
        "        r'i will explore',\n",
        "        r'\\[.*?\\]',\n",
        "        r'\\b(please|kindly|thank you)\\b',\n",
        "        r'\\dth grade',\n",
        "        r'positive attitude is',\n",
        "        r'personal growth and',\n",
        "        r'career at a'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    return ' '.join(text.split()).strip()\n",
        "\n",
        "def augment_data(df, n_samples=2000):\n",
        "    \"\"\"Аугментация данных через смешивание текстов\"\"\"\n",
        "    n_ai_samples = min(n_samples//2, len(df[df['label'] == 1]))\n",
        "    n_human_samples = min(n_samples//2, len(df[df['label'] == 0]))\n",
        "\n",
        "    ai_texts = df[df['label'] == 1]['text'].sample(n_ai_samples).tolist()\n",
        "    human_texts = df[df['label'] == 0]['text'].sample(n_human_samples).tolist()\n",
        "\n",
        "    mixed_samples = []\n",
        "    for ai, human in zip(ai_texts, human_texts):\n",
        "        mixed_ai_human = ai[:len(ai)//2] + human[len(human)//2:]\n",
        "        mixed_human_ai = human[:len(human)//2] + ai[len(ai)//2:]\n",
        "\n",
        "        mixed_samples.extend([\n",
        "            {'text': mixed_ai_human, 'label': 1},\n",
        "            {'text': mixed_human_ai, 'label': 0}\n",
        "        ])\n",
        "\n",
        "    return pd.concat([df, pd.DataFrame(mixed_samples)])\n",
        "\n",
        "def analyze_data(train_df, test_df):\n",
        "    \"\"\"Анализ данных с визуализацией\"\"\"\n",
        "    print(\"\\n=== АНАЛИЗ ДАННЫХ ===\")\n",
        "    print(\"\\nРаспределение меток:\")\n",
        "    print(train_df['label'].value_counts(normalize=True))\n",
        "\n",
        "    train_df['length'] = train_df['text'].apply(len)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.boxplot(x='label', y='length', data=train_df)\n",
        "    plt.title(\"Распределение длины текстов\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZXysq56aE4RV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZMg3u0BElDA"
      },
      "outputs": [],
      "source": [
        "#Модель\n",
        "\n",
        "def train_and_validate():\n",
        "    # Загрузка и очистка данных\n",
        "    train = pd.read_csv(CONFIG['data']['train_path'])\n",
        "    test = pd.read_csv(CONFIG['data']['test_path'])\n",
        "    train['text'] = train['text'].apply(clean_text)\n",
        "    test['text'] = test['text'].apply(clean_text)\n",
        "\n",
        "    # Аугментация данных\n",
        "    train = augment_data(train)\n",
        "    analyze_data(train, test)\n",
        "\n",
        "    # Инициализация TF-IDF и SVD\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=CONFIG['tfidf']['max_features'],\n",
        "        ngram_range=CONFIG['tfidf']['ngram_range'])\n",
        "\n",
        "    svd = TruncatedSVD(n_components=CONFIG['svd']['n_components'])\n",
        "\n",
        "    # Выбор модели\n",
        "    model_type = CONFIG['model']['type']\n",
        "    if model_type == 'logistic':\n",
        "        model = LogisticRegression(**CONFIG['model']['params']['logistic'], random_state=CONFIG['data']['random_state'])\n",
        "    elif model_type == 'random_forest':\n",
        "        model = RandomForestClassifier(**CONFIG['model']['params']['random_forest'], random_state=CONFIG['data']['random_state'])\n",
        "    elif model_type == 'svm':\n",
        "        model = SVC(**CONFIG['model']['params']['svm'], probability=True, random_state=CONFIG['data']['random_state'])\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {model_type}\")\n",
        "\n",
        "    # Создаем pipeline\n",
        "    pipeline = Pipeline([\n",
        "        ('tfidf', tfidf),\n",
        "        ('svd', svd),\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', model)\n",
        "    ])\n",
        "\n",
        "    # Кросс-валидация\n",
        "    skf = StratifiedKFold(\n",
        "        n_splits=CONFIG['data']['n_folds'],\n",
        "        shuffle=True,\n",
        "        random_state=CONFIG['data']['random_state'])\n",
        "\n",
        "    fold_metrics = []\n",
        "    test_preds = np.zeros(len(test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train['text'], train['label'])):\n",
        "        print(f\"\\n=== Fold {fold+1}/{CONFIG['data']['n_folds']} ===\")\n",
        "\n",
        "        # Разделение данных\n",
        "        train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
        "\n",
        "        # Обучение pipeline\n",
        "        pipeline.fit(train_df['text'], train_df['label'])\n",
        "\n",
        "        # Предсказание на валидации\n",
        "        val_preds = pipeline.predict_proba(val_df['text'])[:, 1]\n",
        "\n",
        "        # Метрики\n",
        "        val_labels = val_df['label'].values\n",
        "        auc = roc_auc_score(val_labels, val_preds)\n",
        "        f1 = f1_score(val_labels, (val_preds > 0.5).astype(int))\n",
        "        acc = accuracy_score(val_labels, (val_preds > 0.5).astype(int))\n",
        "        recall_ai = recall_score(val_labels, (val_preds > 0.5).astype(int), pos_label=1)\n",
        "\n",
        "        print(f\"Val AUC: {auc:.4f}\")\n",
        "        print(f\"Val F1: {f1:.4f}\")\n",
        "        print(f\"Val Accuracy: {acc:.4f}\")\n",
        "        print(f\"Val Recall (AI): {recall_ai:.4f}\")\n",
        "\n",
        "        fold_metrics.append(auc)\n",
        "\n",
        "        # Предсказание на тесте\n",
        "        test_preds += pipeline.predict_proba(test['text'])[:, 1] / CONFIG['data']['n_folds']\n",
        "\n",
        "    # Сохранение результатов\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'generated': test_preds\n",
        "    })\n",
        "    submission.to_csv('submission_tfidf_svd.csv', index=False)\n",
        "\n",
        "    print(\"\\n=== ИТОГОВЫЕ МЕТРИКИ ===\")\n",
        "    print(f\"Средний AUC по фолдам: {np.mean(fold_metrics):.4f} (±{np.std(fold_metrics):.4f})\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_and_validate()"
      ],
      "metadata": {
        "id": "eBNLS2x6FJo6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}