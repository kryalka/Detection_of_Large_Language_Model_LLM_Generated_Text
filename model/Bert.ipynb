{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#Импорт библиотек\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, recall_score\n",
        "from tqdm import tqdm\n",
        "import warnings"
      ],
      "metadata": {
        "id": "dHUL7BvTEp6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Отключаем предупреждения\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Конфигурация\n",
        "CONFIG = {\n",
        "    'data': {\n",
        "        'train_path': \"/kaggle/input/train-dataset/train_v2_drcat_02.csv\",\n",
        "        'test_path': \"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\",\n",
        "        'bert_path': \"/kaggle/input/bert-basing\",\n",
        "        'max_length': 128,\n",
        "        'random_state': 42,\n",
        "        'n_folds': 3\n",
        "    },\n",
        "    'model': {\n",
        "        'hidden_dim': 128,\n",
        "        'dropout': 0.3,\n",
        "        'lr': 2e-5\n",
        "    },\n",
        "    'training': {\n",
        "        'batch_size': 32,\n",
        "        'epochs': 3,\n",
        "        'warmup_steps': 100,\n",
        "        'patience': 2\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "8-pHzHDwI7ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Класс Bert\n",
        "class BertDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=128, labels=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0)\n",
        "        }\n",
        "\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.FloatTensor([self.labels[idx]])\n",
        "\n",
        "        return item"
      ],
      "metadata": {
        "id": "e2NBdx6GI577"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Класс Dataset только для BERT\n",
        "class BertDataset(Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=128, labels=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0)\n",
        "        }\n",
        "\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.FloatTensor([self.labels[idx]])\n",
        "\n",
        "        return item"
      ],
      "metadata": {
        "id": "Z2nhJI3DJDTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Модель только с BERT\n",
        "class BertOnlyModel(nn.Module):\n",
        "    def __init__(self, bert_model, hidden_dim=128, dropout=0.3):\n",
        "        super().__init__()\n",
        "        self.bert = bert_model\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(768, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        return self.classifier(pooled_output)"
      ],
      "metadata": {
        "id": "bcDlUsyuJFrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Подготовка датасетов\n",
        "def clean_text(text):\n",
        "    \"\"\"Расширенная очистка текста от артефактов\"\"\"\n",
        "    patterns = [\n",
        "        r'sincerely,\\s*\\[your name\\]',\n",
        "        r'as an (8th|eighth\\-grade) student',\n",
        "        r'(writing|today) to express',\n",
        "        r'hey there! so,',\n",
        "        r'first impressions are',\n",
        "        r'a four\\-day school week',\n",
        "        r'reduce traffic congestion[,\\\\.]',\n",
        "        r'i will explore',\n",
        "        r'\\[.*?\\]',\n",
        "        r'\\b(please|kindly|thank you)\\b',\n",
        "        r'\\dth grade',\n",
        "        r'positive attitude is',\n",
        "        r'personal growth and',\n",
        "        r'career at a'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    return ' '.join(text.split()).strip()\n",
        "\n",
        "def augment_data(df, n_samples=2000):\n",
        "    \"\"\"Аугментация данных через смешивание текстов\"\"\"\n",
        "    n_ai_samples = min(n_samples//2, len(df[df['label'] == 1]))\n",
        "    n_human_samples = min(n_samples//2, len(df[df['label'] == 0]))\n",
        "\n",
        "    ai_texts = df[df['label'] == 1]['text'].sample(n_ai_samples).tolist()\n",
        "    human_texts = df[df['label'] == 0]['text'].sample(n_human_samples).tolist()\n",
        "\n",
        "    mixed_samples = []\n",
        "    for ai, human in zip(ai_texts, human_texts):\n",
        "        mixed_ai_human = ai[:len(ai)//2] + human[len(human)//2:]\n",
        "        mixed_human_ai = human[:len(human)//2] + ai[len(ai)//2:]\n",
        "\n",
        "        mixed_samples.extend([\n",
        "            {'text': mixed_ai_human, 'label': 1},\n",
        "            {'text': mixed_human_ai, 'label': 0}\n",
        "        ])\n",
        "\n",
        "    return pd.concat([df, pd.DataFrame(mixed_samples)])"
      ],
      "metadata": {
        "id": "JfsdQx0-JHvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_loader, device):\n",
        "    \"\"\"Оценка модели\"\"\"\n",
        "    model.eval()\n",
        "    preds, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            outputs = model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask']\n",
        "            )\n",
        "            preds.extend(outputs.cpu().numpy().flatten())\n",
        "            labels.extend(batch['labels'].cpu().numpy().flatten())\n",
        "\n",
        "    preds = np.array(preds)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return {\n",
        "        'auc': roc_auc_score(labels, preds),\n",
        "        'f1': f1_score(labels, (preds > 0.5).astype(int)),\n",
        "        'acc': accuracy_score(labels, (preds > 0.5).astype(int)),\n",
        "        'recall_ai': recall_score(labels, (preds > 0.5).astype(int), pos_label=1)\n",
        "    }\n"
      ],
      "metadata": {
        "id": "LG40yCUoJKeR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_loader, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc='Training'):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask']\n",
        "        )\n",
        "\n",
        "        loss = criterion(outputs, batch['labels'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "def train_and_validate():\n",
        "    # Инициализация\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Загрузка и очистка данных\n",
        "    train = pd.read_csv(CONFIG['data']['train_path'])\n",
        "    test = pd.read_csv(CONFIG['data']['test_path'])\n",
        "    train['text'] = train['text'].apply(clean_text)\n",
        "    test['text'] = test['text'].apply(clean_text)\n",
        "\n",
        "    # Аугментация данных\n",
        "    train = augment_data(train)\n",
        "\n",
        "    # Инициализация токенизатора\n",
        "    tokenizer = AutoTokenizer.from_pretrained(CONFIG['data']['bert_path'], local_files_only=True)\n",
        "\n",
        "    # Кросс-валидация\n",
        "    skf = StratifiedKFold(\n",
        "        n_splits=CONFIG['data']['n_folds'],\n",
        "        shuffle=True,\n",
        "        random_state=CONFIG['data']['random_state'])\n",
        "\n",
        "    fold_metrics = []\n",
        "    test_preds = np.zeros(len(test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train['text'], train['label'])):\n",
        "        print(f\"\\n=== Fold {fold+1}/{CONFIG['data']['n_folds']} ===\")\n",
        "\n",
        "        # Разделение данных\n",
        "        train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
        "\n",
        "        # Даталоадеры\n",
        "        train_dataset = BertDataset(\n",
        "            train_df['text'].tolist(),\n",
        "            tokenizer,\n",
        "            max_length=CONFIG['data']['max_length'],\n",
        "            labels=train_df['label'].tolist())\n",
        "\n",
        "        val_dataset = BertDataset(\n",
        "            val_df['text'].tolist(),\n",
        "            tokenizer,\n",
        "            max_length=CONFIG['data']['max_length'],\n",
        "            labels=val_df['label'].tolist())\n",
        "\n",
        "        test_dataset = BertDataset(\n",
        "            test['text'].tolist(),\n",
        "            tokenizer,\n",
        "            max_length=CONFIG['data']['max_length'])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=CONFIG['training']['batch_size'],\n",
        "            shuffle=True)\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=CONFIG['training']['batch_size'])\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=CONFIG['training']['batch_size'])\n",
        "\n",
        "        # Инициализация модели\n",
        "        bert_model = AutoModel.from_pretrained(CONFIG['data']['bert_path'], local_files_only=True).to(device)\n",
        "        model = BertOnlyModel(\n",
        "            bert_model,\n",
        "            hidden_dim=CONFIG['model']['hidden_dim'],\n",
        "            dropout=CONFIG['model']['dropout']).to(device)\n",
        "\n",
        "        # Оптимизатор\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['model']['lr'])\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=CONFIG['training']['warmup_steps'],\n",
        "            num_training_steps=len(train_loader)*CONFIG['training']['epochs'])\n",
        "\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # Обучение\n",
        "        best_auc = 0\n",
        "        patience = 0\n",
        "\n",
        "        for epoch in range(CONFIG['training']['epochs']):\n",
        "            train_loss = train_epoch(\n",
        "                model, train_loader,\n",
        "                criterion, optimizer, scheduler, device)\n",
        "\n",
        "            val_metrics = evaluate(model, val_loader, device)\n",
        "            print(f\"\\nEpoch {epoch+1}:\")\n",
        "            print(f\"Train Loss: {train_loss:.4f}\")\n",
        "            print(f\"Val AUC: {val_metrics['auc']:.4f}\")\n",
        "            print(f\"Val Recall (AI): {val_metrics['recall_ai']:.4f}\")\n",
        "\n",
        "            # Ранняя остановка\n",
        "            if val_metrics['auc'] > best_auc:\n",
        "                best_auc = val_metrics['auc']\n",
        "                patience = 0\n",
        "                torch.save(model.state_dict(), f'best_bert_model_fold{fold}.pt')\n",
        "            else:\n",
        "                patience += 1\n",
        "                if patience >= CONFIG['training']['patience']:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        fold_metrics.append(best_auc)\n",
        "\n",
        "        # Предсказание на тесте\n",
        "        model.load_state_dict(torch.load(f'best_bert_model_fold{fold}.pt'))\n",
        "        model.eval()\n",
        "\n",
        "        fold_preds = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                outputs = model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask']\n",
        "                )\n",
        "                fold_preds.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "        test_preds += np.array(fold_preds) / CONFIG['data']['n_folds']\n",
        "\n",
        "    # Сохранение результатов\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'generated': test_preds\n",
        "    })\n",
        "    submission.to_csv('submission_bert_only.csv', index=False)\n",
        "\n",
        "    print(\"\\n=== ИТОГОВЫЕ МЕТРИКИ ===\")\n",
        "    print(f\"Средний AUC по фолдам: {np.mean(fold_metrics):.4f} (±{np.std(fold_metrics):.4f})\")\n"
      ],
      "metadata": {
        "id": "7e49WhjBJNif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_and_validate()"
      ],
      "metadata": {
        "id": "L3jgnXpDJSah"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}