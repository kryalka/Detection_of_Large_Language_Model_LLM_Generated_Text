{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiiT7kYEUSFJ"
      },
      "outputs": [],
      "source": [
        "#Импорт библиотек\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, recall_score\n",
        "from transformers import AutoTokenizer, AutoModel, get_linear_schedule_with_warmup\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from collections import defaultdict\n",
        "import warnings"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Отключаем предупреждения\n",
        "warnings.filterwarnings('ignore')\n",
        "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
        "os.environ['HF_DATASETS_OFFLINE'] = '1'\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas(disable=True)"
      ],
      "metadata": {
        "id": "Newd8t8aO7ML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== КОНФИГУРАЦИЯ ====================\n",
        "CONFIG = {\n",
        "    'data': {\n",
        "        'train_path': \"/kaggle/input/train-dataset/train_v2_drcat_02.csv\",\n",
        "        'test_path': \"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\",\n",
        "        'bert_path': \"/kaggle/input/bert-basing\",\n",
        "        'max_length': 128,\n",
        "        'random_state': 42,\n",
        "        'n_folds': 3\n",
        "    },\n",
        "    'tfidf': {\n",
        "        'max_features': 10000,\n",
        "        'ngram_range': (1, 2)\n",
        "    },\n",
        "    'svd': {\n",
        "        'n_components': 300\n",
        "    },\n",
        "    'model': {\n",
        "        'hidden_dim': 128,\n",
        "        'dropout': 0.3,\n",
        "        'lr': {\n",
        "            'model': 1e-4,\n",
        "            'bert': 2e-5\n",
        "        }\n",
        "    },\n",
        "    'training': {\n",
        "        'batch_size': 32,\n",
        "        'epochs': 3,\n",
        "        'warmup_steps': 100,\n",
        "        'patience': 2\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "wLjD3-xaO_JO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== КЛАСС ДАТАСЕТА ====================\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, tfidf_vectorizer, svd, tokenizer, max_length=128, labels=None):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tfidf_vectorizer = tfidf_vectorizer\n",
        "        self.svd = svd\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.texts[idx]\n",
        "\n",
        "        # TF-IDF + SVD фичи\n",
        "        tfidf_features = self.tfidf_vectorizer.transform([text]).toarray()[0]\n",
        "        tfidf_features = self.svd.transform(tfidf_features.reshape(1, -1))[0]\n",
        "\n",
        "        # Токенизация BERT\n",
        "        inputs = self.tokenizer(\n",
        "            text,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {\n",
        "            'tfidf_features': torch.FloatTensor(tfidf_features),\n",
        "            'input_ids': inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': inputs['attention_mask'].squeeze(0)\n",
        "        }\n",
        "\n",
        "        if self.labels is not None:\n",
        "            item['labels'] = torch.FloatTensor([self.labels[idx]])\n",
        "\n",
        "        return item"
      ],
      "metadata": {
        "id": "-XpD7tnXPB7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 1. ПОДГОТОВКА ДАННЫХ ====================\n",
        "def clean_text(text):\n",
        "    \"\"\"Расширенная очистка текста от артефактов\"\"\"\n",
        "    patterns = [\n",
        "        r'sincerely,\\s*\\[your name\\]',\n",
        "        r'as an (8th|eighth\\-grade) student',\n",
        "        r'(writing|today) to express',\n",
        "        r'hey there! so,',\n",
        "        r'first impressions are',\n",
        "        r'a four\\-day school week',\n",
        "        r'reduce traffic congestion[,\\.]',\n",
        "        r'i will explore',\n",
        "        r'\\[.*?\\]',\n",
        "        r'\\b(please|kindly|thank you)\\b',\n",
        "        r'\\dth grade',\n",
        "        r'positive attitude is',\n",
        "        r'personal growth and',\n",
        "        r'career at a'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE)\n",
        "\n",
        "    return ' '.join(text.split()).strip()\n",
        "\n",
        "def augment_data(df, n_samples=2000):\n",
        "    \"\"\"Аугментация данных через смешивание текстов\"\"\"\n",
        "    # Выбираем образцы для каждого класса\n",
        "    n_ai_samples = min(n_samples//2, len(df[df['label'] == 1]))\n",
        "    n_human_samples = min(n_samples//2, len(df[df['label'] == 0]))\n",
        "\n",
        "    ai_texts = df[df['label'] == 1]['text'].sample(n_ai_samples).tolist()\n",
        "    human_texts = df[df['label'] == 0]['text'].sample(n_human_samples).tolist()\n",
        "\n",
        "    mixed_samples = []\n",
        "    for ai, human in zip(ai_texts, human_texts):\n",
        "        # Смешиваем половинки текстов\n",
        "        mixed_ai_human = ai[:len(ai)//2] + human[len(human)//2:]\n",
        "        mixed_human_ai = human[:len(human)//2] + ai[len(ai)//2:]\n",
        "\n",
        "        mixed_samples.extend([\n",
        "            {'text': mixed_ai_human, 'label': 1},\n",
        "            {'text': mixed_human_ai, 'label': 0}\n",
        "        ])\n",
        "\n",
        "    return pd.concat([df, pd.DataFrame(mixed_samples)])\n",
        "\n",
        "def analyze_data(train_df, test_df):\n",
        "    \"\"\"Анализ данных с визуализацией\"\"\"\n",
        "    print(\"\\n=== АНАЛИЗ ДАННЫХ ===\")\n",
        "\n",
        "    # Распределение метокTextDataset\n",
        "    print(\"\\nРаспределение меток:\")\n",
        "    print(train_df['label'].value_counts(normalize=True))\n",
        "\n",
        "    # Длина текстов\n",
        "    train_df['length'] = train_df['text'].apply(len)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    sns.boxplot(x='label', y='length', data=train_df)\n",
        "    plt.title(\"Распределение длины текстов\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "QLVTfS_JPFQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 2. МОДЕЛЬ ====================\n",
        "class EnhancedHybridModel(nn.Module):\n",
        "    \"\"\"Улучшенная гибридная модель\"\"\"\n",
        "    def __init__(self, tfidf_dim, bert_dim, hidden_dim=128, dropout=0.3):\n",
        "        super().__init__()\n",
        "        # Проекции для признаков\n",
        "        self.tfidf_proj1 = nn.Linear(tfidf_dim, hidden_dim)\n",
        "        self.tfidf_proj2 = nn.Sequential(\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        self.bert_proj = nn.Sequential(\n",
        "            nn.Linear(bert_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Механизм внимания (упрощенная версия)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Softmax(dim=0)\n",
        "        )\n",
        "\n",
        "        # Классификатор\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim//2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim//2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, tfidf_features, bert_features):\n",
        "        # Проекции признаков\n",
        "        tfidf_out = self.tfidf_proj1(tfidf_features)\n",
        "        tfidf_out = self.tfidf_proj2(tfidf_out)\n",
        "        bert_out = self.bert_proj(bert_features)\n",
        "\n",
        "        # Комбинирование через attention\n",
        "        combined = torch.stack([tfidf_out, bert_out], dim=1)\n",
        "        attn_weights = self.attention(combined)\n",
        "        attended = (combined * attn_weights).sum(dim=1)\n",
        "\n",
        "        return self.classifier(attended)"
      ],
      "metadata": {
        "id": "RgiNPfnsPHQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==================== 3. ОБУЧЕНИЕ ====================\n",
        "def evaluate(model, val_loader, bert_model, device):\n",
        "    \"\"\"Расширенная оценка модели\"\"\"\n",
        "    model.eval()\n",
        "    bert_model.eval()\n",
        "    preds, labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: v.to(device) for k, v in batch.items()}\n",
        "            bert_outputs = bert_model(\n",
        "                input_ids=batch['input_ids'],\n",
        "                attention_mask=batch['attention_mask'])\n",
        "\n",
        "            outputs = model(\n",
        "                batch['tfidf_features'],\n",
        "                bert_outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "            preds.extend(outputs.cpu().numpy().flatten())\n",
        "            labels.extend(batch['labels'].cpu().numpy().flatten())\n",
        "\n",
        "    preds = np.array(preds)\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    return {\n",
        "        'auc': roc_auc_score(labels, preds),\n",
        "        'f1': f1_score(labels, (preds > 0.5).astype(int)),\n",
        "        'acc': accuracy_score(labels, (preds > 0.5).astype(int)),\n",
        "        'recall_ai': recall_score(labels, (preds > 0.5).astype(int), pos_label=1)\n",
        "    }\n",
        "\n",
        "def train_epoch(model, train_loader, bert_model, criterion, optimizer, scheduler, device):\n",
        "    \"\"\"Одна эпоха обучения\"\"\"\n",
        "    model.train()\n",
        "    bert_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc='Training'):\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        bert_outputs = bert_model(\n",
        "            input_ids=batch['input_ids'],\n",
        "            attention_mask=batch['attention_mask'])\n",
        "\n",
        "        outputs = model(\n",
        "            batch['tfidf_features'],\n",
        "            bert_outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "        loss = criterion(outputs, batch['labels'])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)"
      ],
      "metadata": {
        "id": "TYaJsrNpPMJ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_and_validate():\n",
        "    # Инициализация\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Загрузка и очистка данных\n",
        "    train = pd.read_csv(CONFIG['data']['train_path'])\n",
        "    test = pd.read_csv(CONFIG['data']['test_path'])\n",
        "    train['text'] = train['text'].apply(clean_text)\n",
        "    test['text'] = test['text'].apply(clean_text)\n",
        "\n",
        "    # Аугментация данных\n",
        "    train = augment_data(train)\n",
        "    analyze_data(train, test)\n",
        "\n",
        "    # Инициализация TF-IDF и SVD\n",
        "    tfidf = TfidfVectorizer(\n",
        "        max_features=CONFIG['tfidf']['max_features'],\n",
        "        ngram_range=CONFIG['tfidf']['ngram_range'])\n",
        "    svd = TruncatedSVD(n_components=CONFIG['svd']['n_components'])\n",
        "\n",
        "    # Кросс-валидация\n",
        "    skf = StratifiedKFold(\n",
        "        n_splits=CONFIG['data']['n_folds'],\n",
        "        shuffle=True,\n",
        "        random_state=CONFIG['data']['random_state'])\n",
        "\n",
        "    fold_metrics = []\n",
        "    test_preds = np.zeros(len(test))\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(train['text'], train['label'])):\n",
        "        print(f\"\\n=== Fold {fold+1}/{CONFIG['data']['n_folds']} ===\")\n",
        "\n",
        "        # Разделение данных\n",
        "        train_df, val_df = train.iloc[train_idx], train.iloc[val_idx]\n",
        "\n",
        "        # Обучение TF-IDF и SVD\n",
        "        tfidf_matrix = tfidf.fit_transform(train_df['text'])\n",
        "        svd.fit(tfidf_matrix)\n",
        "\n",
        "        # Даталоадеры\n",
        "        tokenizer = AutoTokenizer.from_pretrained(CONFIG['data']['bert_path'], local_files_only=True)\n",
        "\n",
        "        train_dataset = TextDataset(\n",
        "            train_df['text'].tolist(),\n",
        "            tfidf, svd, tokenizer,\n",
        "            max_length=CONFIG['data']['max_length'],\n",
        "            labels=train_df['label'].tolist())\n",
        "\n",
        "        val_dataset = TextDataset(\n",
        "            val_df['text'].tolist(),\n",
        "            tfidf, svd, tokenizer,\n",
        "            max_length=CONFIG['data']['max_length'],\n",
        "            labels=val_df['label'].tolist())\n",
        "\n",
        "        test_dataset = TextDataset(\n",
        "            test['text'].tolist(),\n",
        "            tfidf, svd, tokenizer,\n",
        "            max_length=CONFIG['data']['max_length'])\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size=CONFIG['training']['batch_size'],\n",
        "            shuffle=True)\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_dataset,\n",
        "            batch_size=CONFIG['training']['batch_size'])\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=CONFIG['training']['batch_size'])\n",
        "\n",
        "        # Инициализация моделей\n",
        "        bert_model = AutoModel.from_pretrained(CONFIG['data']['bert_path'], local_files_only=True).to(device)\n",
        "        model = EnhancedHybridModel(\n",
        "            tfidf_dim=CONFIG['svd']['n_components'],\n",
        "            bert_dim=768,\n",
        "            hidden_dim=CONFIG['model']['hidden_dim'],\n",
        "            dropout=CONFIG['model']['dropout']).to(device)\n",
        "\n",
        "        # Оптимизатор с разными learning rates\n",
        "        optimizer = torch.optim.AdamW([\n",
        "            {'params': model.tfidf_proj1.parameters(), 'lr': CONFIG['model']['lr']['model']},\n",
        "            {'params': model.tfidf_proj2.parameters(), 'lr': CONFIG['model']['lr']['model']},\n",
        "            {'params': model.bert_proj.parameters(), 'lr': CONFIG['model']['lr']['bert']},\n",
        "            {'params': model.attention.parameters(), 'lr': CONFIG['model']['lr']['model']},\n",
        "            {'params': model.classifier.parameters(), 'lr': CONFIG['model']['lr']['model']}\n",
        "        ])\n",
        "\n",
        "        scheduler = get_linear_schedule_with_warmup(\n",
        "            optimizer,\n",
        "            num_warmup_steps=CONFIG['training']['warmup_steps'],\n",
        "            num_training_steps=len(train_loader)*CONFIG['training']['epochs'])\n",
        "\n",
        "        criterion = nn.BCELoss()\n",
        "\n",
        "        # Обучение\n",
        "        best_auc = 0\n",
        "        patience = 0\n",
        "\n",
        "        for epoch in range(CONFIG['training']['epochs']):\n",
        "            train_loss = train_epoch(\n",
        "                model, train_loader, bert_model,\n",
        "                criterion, optimizer, scheduler, device)\n",
        "\n",
        "            val_metrics = evaluate(model, val_loader, bert_model, device)\n",
        "            print(f\"\\nEpoch {epoch+1}:\")\n",
        "            print(f\"Train Loss: {train_loss:.4f}\")\n",
        "            print(f\"Val AUC: {val_metrics['auc']:.4f}\")\n",
        "            print(f\"Val Recall (AI): {val_metrics['recall_ai']:.4f}\")\n",
        "\n",
        "            # Ранняя остановка\n",
        "            if val_metrics['auc'] > best_auc:\n",
        "                best_auc = val_metrics['auc']\n",
        "                patience = 0\n",
        "                torch.save(model.state_dict(), f'best_model_fold{fold}.pt')\n",
        "            else:\n",
        "                patience += 1\n",
        "                if patience >= CONFIG['training']['patience']:\n",
        "                    print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                    break\n",
        "\n",
        "        fold_metrics.append(best_auc)\n",
        "\n",
        "        # Предсказание на тесте\n",
        "        model.load_state_dict(torch.load(f'best_model_fold{fold}.pt'))\n",
        "        model.eval()\n",
        "\n",
        "        fold_preds = []\n",
        "        with torch.no_grad():\n",
        "            for batch in test_loader:\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "                bert_outputs = bert_model(\n",
        "                    input_ids=batch['input_ids'],\n",
        "                    attention_mask=batch['attention_mask'])\n",
        "\n",
        "                outputs = model(\n",
        "                    batch['tfidf_features'],\n",
        "                    bert_outputs.last_hidden_state[:, 0, :])\n",
        "\n",
        "                fold_preds.extend(outputs.cpu().numpy().flatten())\n",
        "\n",
        "        test_preds += np.array(fold_preds) / CONFIG['data']['n_folds']\n",
        "    # Сохранение результатов\n",
        "    submission = pd.DataFrame({\n",
        "        'id': test['id'],\n",
        "        'generated': test_preds\n",
        "    })\n",
        "    submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "    print(\"\\n=== ИТОГОВЫЕ МЕТРИКИ ===\")\n",
        "    print(f\"Средний AUC по фолдам: {np.mean(fold_metrics):.4f} (±{np.std(fold_metrics):.4f})\")"
      ],
      "metadata": {
        "id": "gfFJsWkNPakK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_and_validate()"
      ],
      "metadata": {
        "id": "MaZiKN-EPk97"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}